experiment_name: "lht_pretrain_base"

seed: 42
device: "cuda"

model:
  vocab_size: 32000
  d_model: 768
  n_heads: 12
  n_layers_local: 3        # layers 1–3
  n_layers_mid: 3          # layers 4–6
  n_layers_global: 6       # layers 7–12
  d_ff: 3072
  dropout: 0.1
  max_seq_len: 4096
  rope: true

router:
  hidden_dim: 256
  window_size: 5          # conv context
  token_to_sentence:
    target_head_ratio: 0.03
    loss_weight: 0.05
  sentence_to_section:
    target_head_ratio: 0.15
    loss_weight: 0.05
  use_gumbel_ste: true

attention:
  local_window_tokens: 128
  neighbour_sentences: 1
  neighbour_sections: 1
  use_doc_token: true

training:
  task: "mlm"              # or "causal_lm"
  batch_size: 8
  grad_accum_steps: 4
  num_steps: 100000
  warmup_steps: 2000
  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  log_every: 100
  save_every: 5000
  eval_every: 5000
  mixed_precision: "bf16"  # or "fp16" / "none"

data:
  dataset_name: "howey/wiki_en"  # or your own
  text_column: "text"
  num_workers: 4
  shuffle_buffer_size: 10000
  max_seq_len: 4096
  tokenizer_name_or_path: "gpt2"
  pretokenized: false
